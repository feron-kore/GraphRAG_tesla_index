{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 7604, Requested 283. Please try again in 34.645s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 7604, Requested 283. Please try again in 34.645s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"HERTZ GLOBAL HOLDINGS INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Hertz Global Holdings Inc and has an employee, director or consultant of Morgan Stanley who is a director of Hertz Global Holdings Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Hertz Global Holdings Inc and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 7603, Requested 251. Please try again in 34.254s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 7603, Requested 251. Please try again in 34.254s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LEAR CORPORATION\\\"\"]\nDescription List: [\"\\\"Morgan Stanley has received compensation for products and services other than investment banking services from Lear Corporation.\\\"\", \"\\\"Morgan Stanley provides research coverage for Lear Corporation and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8900, Requested 254. Please try again in 49.849999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8900, Requested 254. Please try again in 49.849999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LI-CYCLE HOLDINGS CORP.\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Li-Cycle Holdings Corp.\\\"\", \"\\\"Morgan Stanley provides research coverage for Li-Cycle Holdings Corp. and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 9217, Requested 251. Please try again in 53.625s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 9217, Requested 251. Please try again in 53.625s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LITHIA MOTORS INC.\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Lithia Motors Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Lithia Motors Inc. and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 9563, Requested 250. Please try again in 57.76s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 9563, Requested 250. Please try again in 57.76s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"QUANTUMSCAPE CORP\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Quantumscape Corp.\\\"\", \"\\\"Morgan Stanley provides research coverage for Quantumscape Corp and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 9563, Requested 271. Please try again in 58.012s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 9563, Requested 271. Please try again in 58.012s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"MOBILEYE GLOBAL INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Mobileye Global Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Mobileye Global Inc, managed or co-managed a public offering of securities in the last 12 months, and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11458, Requested 251. Please try again in 1m20.51s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11458, Requested 251. Please try again in 1m20.51s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LEAR CORPORATION\\\"\"]\nDescription List: [\"\\\"Morgan Stanley has received compensation for products and services other than investment banking services from Lear Corporation.\\\"\", \"\\\"Morgan Stanley provides research coverage for Lear Corporation and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11359, Requested 283. Please try again in 1m19.708s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11359, Requested 283. Please try again in 1m19.708s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"HERTZ GLOBAL HOLDINGS INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Hertz Global Holdings Inc and has an employee, director or consultant of Morgan Stanley who is a director of Hertz Global Holdings Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Hertz Global Holdings Inc and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11264, Requested 251. Please try again in 1m18.189s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11264, Requested 251. Please try again in 1m18.189s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LITHIA MOTORS INC.\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Lithia Motors Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Lithia Motors Inc. and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11171, Requested 250. Please try again in 1m17.055s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11171, Requested 250. Please try again in 1m17.055s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"QUANTUMSCAPE CORP\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Quantumscape Corp.\\\"\", \"\\\"Morgan Stanley provides research coverage for Quantumscape Corp and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11069, Requested 271. Please try again in 1m16.084s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 11069, Requested 271. Please try again in 1m16.084s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"MOBILEYE GLOBAL INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Mobileye Global Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Mobileye Global Inc, managed or co-managed a public offering of securities in the last 12 months, and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 10973, Requested 254. Please try again in 1m14.725s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 10973, Requested 254. Please try again in 1m14.725s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LI-CYCLE HOLDINGS CORP.\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Li-Cycle Holdings Corp.\\\"\", \"\\\"Morgan Stanley provides research coverage for Li-Cycle Holdings Corp. and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 10885, Requested 251. Please try again in 1m13.64s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 10885, Requested 251. Please try again in 1m13.64s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LEAR CORPORATION\\\"\"]\nDescription List: [\"\\\"Morgan Stanley has received compensation for products and services other than investment banking services from Lear Corporation.\\\"\", \"\\\"Morgan Stanley provides research coverage for Lear Corporation and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error executing verb \"summarize_descriptions\" in create_summarized_entities: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 10885, Requested 251. Please try again in 1m13.64s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\datashaper\\workflow\\workflow.py\", line 415, in _execute_verb\n    result = await result\n             ^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\description_summarize.py\", line 183, in summarize_descriptions\n    results = [\n              ^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\description_summarize.py\", line 184, in <listcomp>\n    await get_resolved_entities(row, semaphore) for row in output.itertuples()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\description_summarize.py\", line 147, in get_resolved_entities\n    results = await asyncio.gather(*futures)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 349, in __wakeup\n    future.result()\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 277, in __step\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\description_summarize.py\", line 167, in do_summarize_descriptions\n    results = await strategy_exec(\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\strategies\\graph_intelligence\\run_graph_intelligence.py\", line 34, in run\n    return await run_summarize_descriptions(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\strategies\\graph_intelligence\\run_graph_intelligence.py\", line 67, in run_summarize_descriptions\n    result = await extractor(items=items, descriptions=descriptions)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\graph\\extractors\\summarize\\description_summary_extractor.py\", line 73, in __call__\n    result = await self._summarize_descriptions(items, descriptions)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\graph\\extractors\\summarize\\description_summary_extractor.py\", line 106, in _summarize_descriptions\n    result = await self._summarize_descriptions_with_llm(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\graph\\extractors\\summarize\\description_summary_extractor.py\", line 125, in _summarize_descriptions_with_llm\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 10885, Requested 251. Please try again in 1m13.64s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 10885, Requested 251. Please try again in 1m13.64s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": null}
{"type": "error", "data": "Error running pipeline!", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\run.py\", line 323, in run_pipeline\n    result = await workflow.run(context, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\datashaper\\workflow\\workflow.py\", line 369, in run\n    timing = await self._execute_verb(node, context, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\datashaper\\workflow\\workflow.py\", line 415, in _execute_verb\n    result = await result\n             ^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\description_summarize.py\", line 183, in summarize_descriptions\n    results = [\n              ^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\description_summarize.py\", line 184, in <listcomp>\n    await get_resolved_entities(row, semaphore) for row in output.itertuples()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\description_summarize.py\", line 147, in get_resolved_entities\n    results = await asyncio.gather(*futures)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 349, in __wakeup\n    future.result()\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 277, in __step\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\description_summarize.py\", line 167, in do_summarize_descriptions\n    results = await strategy_exec(\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\strategies\\graph_intelligence\\run_graph_intelligence.py\", line 34, in run\n    return await run_summarize_descriptions(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\verbs\\entities\\summarize\\strategies\\graph_intelligence\\run_graph_intelligence.py\", line 67, in run_summarize_descriptions\n    result = await extractor(items=items, descriptions=descriptions)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\graph\\extractors\\summarize\\description_summary_extractor.py\", line 73, in __call__\n    result = await self._summarize_descriptions(items, descriptions)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\graph\\extractors\\summarize\\description_summary_extractor.py\", line 106, in _summarize_descriptions\n    result = await self._summarize_descriptions_with_llm(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\graph\\extractors\\summarize\\description_summary_extractor.py\", line 125, in _summarize_descriptions_with_llm\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 10885, Requested 251. Please try again in 1m13.64s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 10885, Requested 251. Please try again in 1m13.64s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": null}
