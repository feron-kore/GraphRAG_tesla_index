{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8154, Requested 303. Please try again in 41.487s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8154, Requested 303. Please try again in 41.487s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: \"\\\"FERRARI NV\\\"\"\nDescription List: [\"\\\"Ferrari NV is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley beneficially owned 1% or more of a class of common equity securities as of December 30, 2022.\\\"\\\"Ferrari NV is an organization that is part of Morgan Stanley's research coverage and for which Diego Ortega Laya owns securities.\\\"\", \"\\\"Ferrari NV is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 254. Please try again in 40.893s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 254. Please try again in 40.893s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LI-CYCLE HOLDINGS CORP.\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Li-Cycle Holdings Corp.\\\"\", \"\\\"Morgan Stanley provides research coverage for Li-Cycle Holdings Corp. and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8154, Requested 251. Please try again in 40.862s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8154, Requested 251. Please try again in 40.862s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LEAR CORPORATION\\\"\"]\nDescription List: [\"\\\"Morgan Stanley has received compensation for products and services other than investment banking services from Lear Corporation.\\\"\", \"\\\"Morgan Stanley provides research coverage for Lear Corporation and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 283. Please try again in 41.24s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 283. Please try again in 41.24s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"HERTZ GLOBAL HOLDINGS INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Hertz Global Holdings Inc and has an employee, director or consultant of Morgan Stanley who is a director of Hertz Global Holdings Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Hertz Global Holdings Inc and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 271. Please try again in 41.094s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 271. Please try again in 41.094s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"MOBILEYE GLOBAL INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Mobileye Global Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Mobileye Global Inc, managed or co-managed a public offering of securities in the last 12 months, and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 250. Please try again in 40.844s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 250. Please try again in 40.844s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"QUANTUMSCAPE CORP\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Quantumscape Corp.\\\"\", \"\\\"Morgan Stanley provides research coverage for Quantumscape Corp and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 251. Please try again in 40.852s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8153, Requested 251. Please try again in 40.852s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LITHIA MOTORS INC.\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Lithia Motors Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Lithia Motors Inc. and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8045, Requested 251. Please try again in 39.56s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8045, Requested 251. Please try again in 39.56s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LEAR CORPORATION\\\"\"]\nDescription List: [\"\\\"Morgan Stanley has received compensation for products and services other than investment banking services from Lear Corporation.\\\"\", \"\\\"Morgan Stanley provides research coverage for Lear Corporation and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8034, Requested 271. Please try again in 39.669s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8034, Requested 271. Please try again in 39.669s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"MOBILEYE GLOBAL INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Mobileye Global Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Mobileye Global Inc, managed or co-managed a public offering of securities in the last 12 months, and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8032, Requested 303. Please try again in 40.023s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 8032, Requested 303. Please try again in 40.023s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: \"\\\"FERRARI NV\\\"\"\nDescription List: [\"\\\"Ferrari NV is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley beneficially owned 1% or more of a class of common equity securities as of December 30, 2022.\\\"\\\"Ferrari NV is an organization that is part of Morgan Stanley's research coverage and for which Diego Ortega Laya owns securities.\\\"\", \"\\\"Ferrari NV is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 7518, Requested 283. Please try again in 33.613s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 7518, Requested 283. Please try again in 33.613s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"HERTZ GLOBAL HOLDINGS INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Hertz Global Holdings Inc and has an employee, director or consultant of Morgan Stanley who is a director of Hertz Global Holdings Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Hertz Global Holdings Inc and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 7017, Requested 250. Please try again in 27.206s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 7017, Requested 250. Please try again in 27.206s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"QUANTUMSCAPE CORP\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Quantumscape Corp.\\\"\", \"\\\"Morgan Stanley provides research coverage for Quantumscape Corp and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 6514, Requested 254. Please try again in 21.223s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 6514, Requested 254. Please try again in 21.223s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LI-CYCLE HOLDINGS CORP.\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Li-Cycle Holdings Corp.\\\"\", \"\\\"Morgan Stanley provides research coverage for Li-Cycle Holdings Corp. and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 6015, Requested 251. Please try again in 15.201999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 6015, Requested 251. Please try again in 15.201999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"LITHIA MOTORS INC.\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Lithia Motors Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Lithia Motors Inc. and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 5515, Requested 271. Please try again in 9.435999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 5515, Requested 271. Please try again in 9.435999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"MOBILEYE GLOBAL INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Mobileye Global Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Mobileye Global Inc, managed or co-managed a public offering of securities in the last 12 months, and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1295, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1620, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 5121, Requested 271. Please try again in 4.712s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01j3zbq1gvf3yan94zhpzs41k9` on tokens per minute (TPM): Limit 5000, Used 5121, Requested 271. Please try again in 4.712s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we the have full context.\n\n#######\n-Data-\nEntities: [\"\\\"MORGAN STANLEY\\\"\", \"\\\"MOBILEYE GLOBAL INC\\\"\"]\nDescription List: [\"\\\"Morgan Stanley expects to receive or intends to seek compensation for investment banking services from Mobileye Global Inc.\\\"\", \"\\\"Morgan Stanley provides research coverage for Mobileye Global Inc, managed or co-managed a public offering of securities in the last 12 months, and expects to receive or intends to seek compensation for investment banking services.\\\"\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"BORGWARNER INC.\":BorgWarner Inc. is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. This company is one of the entities mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"CARVANA CO\":Carvana Co is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. The company is one of the entities mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"FISKER INC\":Fisker Inc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. This company is one of those mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"FORD MOTOR COMPANY\":Ford Motor Company is an organization that is part of Morgan Stanley's research coverage, for which Morgan Stanley managed or co-managed a public offering of securities in the last 12 months and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. Additionally, Ford Motor Company is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.", "\"HERTZ GLOBAL HOLDINGS INC\":Hertz Global Holdings Inc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. It is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley and having a relationship with an employee, director, or consultant of Morgan Stanley.", "\"LEAR CORPORATION\":Lear Corporation is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. Additionally, Lear Corporation is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.", "\"LI-CYCLE HOLDINGS CORP.\":Li-Cycle Holdings Corp. is an organization that is both part of Morgan Stanley's research coverage and one of the companies mentioned in the text. Morgan Stanley expects to receive or intends to seek compensation for investment banking services in relation to Li-Cycle Holdings Corp. This information indicates that there is a business relationship between the two entities, with Morgan Stanley providing investment banking services and conducting research on Li-Cycle Holdings Corp.", "\"LITHIA MOTORS INC.\":Lithia Motors Inc. is an organization that is part of Morgan Stanley's research coverage and is one of the companies mentioned in the text. Lithia Motors Inc. is expected to receive or has sought compensation for investment banking services from Morgan Stanley.", "\"MOBILEYE GLOBAL INC\":Mobileye Global Inc is an organization that is part of Morgan Stanley's research coverage, and it has been involved with Morgan Stanley in a public offering of securities within the last 12 months for which Morgan Stanley managed or co-managed and expects to receive or intends to seek compensation for investment banking services. The company is mentioned multiple times in the text, and it is one of the companies potentially relevant to Morgan Stanley's services, possibly receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"QUANTUMSCAPE CORP\":Quantumscape Corp is an organization that is part of Morgan Stanley's research coverage and is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley. This indicates that Quantumscape Corp has a business relationship with Morgan Stanley, where the financial institution provides investment banking services and possibly research coverage, and Quantumscape Corp may receive or seek compensation for these services.", "\"ERTZ GLOBAL HOLDINGS INC\":\"ertz Global Holdings Inc is one of the companies mentioned in the text, potentially relevant to Morgan Stanley's services.\"", "\"TESLA INC.\":\"Tesla Inc. is one of the companies mentioned in the text, potentially relevant to Morgan Stanley's services.\"", "\"LUCID GROUP INC\":\"Lucid Group Inc is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.\"", "\"MAGNA INTERNATIONAL INC.\":\"Magna International Inc. is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.\"", "\"RIVIAN AUTOMOTIVE, INC.\":\"Rivian Automotive, Inc. is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.\"", "\"VISTEON CORPORATION\":\"Visteon Corporation is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.\""]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"MORGAN STANLEY RESEARCH\":\"Morgan Stanley Research is a division of Morgan Stanley that provides the stock analysis in the text.\">)**", "\"MORGAN STANLEY INSTITUTIONAL EQUITIES DIVISION\":\"Morgan Stanley Institutional Equities Division is a division of Morgan Stanley that is mentioned in the text.\">)**", "\"JAN '22\":\"JAN '22 is a point in time, referenced as the start of the historical stock performance data.\">)**", "\"JUL '22\":\"JUL '22 is a point in time, referenced as a date for a stock price.\">)**", "\"JAN '23\":\"JAN '23 is a point in time, referenced as the current stock price.\">)**", "\"JAN '24\":\"JAN '24 is a point in time, referenced as the price target.\">)**", "\"$70.00\":\"$70.00 is a stock price, referenced as a historical stock price and a bear case value.\">)**", "\"$160.27\":\"$160.27 is a stock price, referenced as the stock price at the time of the analysis.\">)**", "\"$390.00\":\"$390.00 is a stock price, referenced as the bull case value.\">)**", "\"$220.00\":\"$220.00 is a stock price, referenced as the base case value and the price target.\">)**", "\"MS RATING\":\"MS Rating is a concept, referenced as a rating given by Morgan Stanley Research.\">)**", "\"EQUAL-WEIGHT\":EQUAL-WEIGHT is a concept referred to in the context of stock portfolio allocation, as well as a stock rating assigned by Morgan Stanley. As a portfolio allocation concept, equal-weight refers to a strategy where an investor divides their investment equally among the securities in their portfolio. On the other hand, when used as a stock rating by Morgan Stanley, equal-weight indicates that the stock's total return is expected to be in line with the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.", "\"UNDERWEIGHT\":Underweight is a concept used in the context of stock portfolio allocation, and it is also a stock rating assigned by Morgan Stanley. As a portfolio allocation concept, underweight refers to a situation where an investor holds less of a particular stock or industry in their portfolio compared to its benchmark index. On the other hand, when used as a stock rating by Morgan Stanley, underweight indicates that the stock's total return is expected to be below the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.", "\"PROB (>220.00)\":\"Prob (>220.00) is a concept, referenced as the probability of the stock price exceeding $220.00.\">)**", "\"PROB (<70.00)\":\"Prob (<70.00) is a concept, referenced as the probability of the stock price being below $70.00.\">)**", "\"DISRUPTION\":\"Disruption is a concept, referenced as a risk reward theme.\">)**"]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"MORGAN STANLEY & CO. LLC\":\"Morgan Stanley & Co. LLC is an organization that has provided an analysis of Tesla Inc's price cuts.\"", "\"TESLA INC\":Tesla Inc is an electric vehicle company and an organization that is part of Morgan Stanley's research coverage. Recently, Tesla Inc reduced its prices across a significant portion of its product range. Morgan Stanley expects to receive or intends to seek compensation for investment banking services for Tesla Inc. The company is also involved in the production of electric vehicles and energy products, and is the primary focus of analysis in the given text.", "\"NORTH AMERICA\":\"North America is the continent where Tesla Inc operates.\"", "\"ADAM JONAS, CFA\":\"Adam Jonas, CFA is an equity analyst who provided a stock rating, industry view, and price target for Tesla Inc.\"", "\"EVAN SILVERBERG, CFA, CPA\":\"Evan Silverberg, CFA, CPA is a research associate who contributed to the analysis of Tesla Inc.\"", "\"MODEL T\":\"Model T is a historical car model produced by Ford that underwent significant price deflation due to manufacturing innovations.\"", "\"HENRY FORD\":Henry Ford is a significant historical figure, known for his pioneering contributions to the manufacturing industry. In 1910, he introduced mass manufacturing at his Highland Park, Michigan factory, and three years later, in 1913, he unveiled the groundbreaking moving assembly line for the Model T. These innovations led to substantial levels of deflation in the automobile industry, demonstrating Ford's transformative impact on the sector. In summary, Henry Ford is associated with the introduction of mass manufacturing and the moving assembly line for the Model T, which brought about dramatic changes and efficiencies in the automotive industry.", "\"TESLA\":\"Tesla is an electric vehicle company that has recently reduced the price of its Model Y, making it competitive with a Toyota RAV4 Hybrid in terms of price.\"", "\"TOYOTA\":\"Toyota is a car manufacturer that produces the RAV4 Hybrid, which is a competitor to Tesla's Model Y in terms of price and features.\"", "\"HIGHLAND PARK, MICHIGAN\":\"Highland Park, Michigan is a location where Henry Ford introduced mass manufacturing in his factory in 1910.\"", "\"1910\":\"1910 is the year when Henry Ford introduced mass manufacturing in his Highland Park, Michigan factory.\"", "\"1913\":\"1913 is the year when Henry Ford introduced his world-changing moving assembly in his factory.\"", "\"MASS MANUFACTURING\":\"Mass Manufacturing is a concept introduced by Henry Ford in his Highland Park, Michigan factory in 1910, driving dramatic levels of deflation in the automobile industry.\"", "\"MOVING ASSEMBLY\":\"Moving Assembly is a concept introduced by Henry Ford in his factory in 1913, driving dramatic levels of deflation in the automobile industry.\"", "\"DRASTIC LEVELS OF DEFLATION\":\"Drastic Levels of Deflation is a concept that refers to the significant reduction in prices in the automobile industry due to innovation in manufacturing.\"", "\"REFINITIV\":\"Refinitiv is a source of data and information for the stock analysis provided in the text.\">)**"]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"SECULAR GROWTH\":\"Secular Growth is a concept, referenced as a risk reward theme.\">)**", "\"ELECTRIC VEHICLES\":\"Electric Vehicles is a concept, referenced as a risk reward theme.\">)**", "\"HISTORICAL STOCK PERFORMANCE\":", "\"CURRENT STOCK PRICE\":", "\"PRICE TARGET\":", "\"BULL CASE\":", "\"BASE CASE\":", "\"RISK REWARD THEMES\":", "\"MORGAN STANLEY\":Morgan Stanley is a financial services company that produces equity research and provides research, ratings, and investment services to clients. The company is subject to certain terms of use and privacy policies. As an investment bank, it corresponds its ratings to buy, hold, and sell recommendations for regulatory purposes. Morgan Stanley is also an organization that offers research, estimates, and opinions about various companies and sectors. It is the entity that has relationships with and provides services to multiple companies mentioned in the text.", "\"DIEGO ORTEGA LAYA\":\"Diego Ortega Laya is a person who owns securities of Ferrari NV and General Motors Company.\"", "\"FERRARI NV\":Ferrari NV is an organization that is part of Morgan Stanley's research coverage. As of December 30, 2022, Morgan Stanley beneficially owned 1% or more of a class of common equity securities of Ferrari NV. Additionally, Diego Ortega Laya owns securities of the company. Ferrari NV is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.", "\"GENERAL MOTORS COMPANY\":General Motors Company is an organization that is part of Morgan Stanley's research coverage. Morgan Stanley managed or co-managed a public offering of securities for General Motors in the last 12 months and beneficially owned 1% or more of a class of common equity securities as of December 30, 2022. Diego Ortega Laya owns securities in the company. Additionally, General Motors is one of the companies mentioned in the text that potentially receives compensation for products and services other than investment banking services from Morgan Stanley, and has a relationship with an employee, director, or consultant of Morgan Stanley.", "\"AMERICAN AXLE & MANUFACTURING HOLDINGS INC\":American Axle & Manufacturing Holdings Inc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. This company is one of those mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"APTIV PLC\":Aptiv Plc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. It is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"AUTONATION INC.\":AutoNation Inc. is an organization that is part of Morgan Stanley's research coverage and is one of the companies mentioned in the text, for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services.", "\"AVIS BUDGET GROUP INC\":Avis Budget Group Inc is an organization that is part of Morgan Stanley's research coverage. In the past 12 months, Morgan Stanley has managed or co-managed a public offering of securities for Avis Budget Group Inc, and Morgan Stanley expects to receive or intends to seek compensation for investment banking services from the company. This information is based on the text that mentions Avis Budget Group Inc as one of the companies potentially receiving or seeking compensation for investment banking services from Morgan Stanley."]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"INVESTMENT BANKING CLIENTS (IBC)\":\"Investment Banking Clients (IBC) are organizations that have paid investment banking fees to Morgan Stanley in the past 12 months.\"", "\"OTHER MATERIAL INVESTMENT SERVICES CLIENTS (MISC)\":\"Other Material Investment Services Clients (MISC) are organizations that receive other investment services from Morgan Stanley but have not paid investment banking fees in the past 12 months.\"", "\"OVERWEIGHT\":\"Overweight is a stock rating assigned by Morgan Stanley, indicating that the stock's total return is expected to exceed the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.\"", "\"NOT-RATED\":\"Not-Rated is a stock rating assigned by Morgan Stanley, indicating that the analyst does not have adequate conviction about the stock's total return relative to the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.\"", "\"BUY\":", "\"HOLD\":", "\"SELL\":", "\"RESEARCH ANALYST\":\"The Research Analyst is a person who contributes to the production of Morgan Stanley Research, with their work typically updated with a certain frequency.\"", "\"RESEARCH MANAGEMENT\":\"Research Management refers to the individuals who determine the publication schedule for Morgan Stanley Research based on current conditions.\"", "\"SECTION 975 OF THE DODD-FRANK WALL STREET REFORM AND CONSUMER PROTECTION ACT\":\"Section 975 is a legislative event that defines the meaning of advice within its context.\"", "\"TACTICAL IDEA\":\"A Tactical Idea is an equity research product from Morgan Stanley that may contain views contrary to recommendations or views expressed in other research on the same stock.\"", "\"MATRIX\":\"Matrix is a proprietary research portal from Morgan Stanley where clients can access research.\"", "\"SALES REPRESENTATIVE\":\"A Sales Representative is a person who can assist clients in accessing Morgan Stanley Research and related products.\"", "\"THIRD-PARTY VENDORS\":\"Third-Party Vendors are organizations that distribute Morgan Stanley Research to clients as a convenience.\""]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"MORGAN STANLEY & CO. LLC\":\"Morgan Stanley & Co. LLC is an organization that has provided an analysis of Tesla Inc's price cuts.\"", "\"TESLA INC\":Tesla Inc is an electric vehicle company and an organization that is part of Morgan Stanley's research coverage. Recently, Tesla Inc reduced its prices across a significant portion of its product range. Morgan Stanley expects to receive or intends to seek compensation for investment banking services for Tesla Inc. The company is also involved in the production of electric vehicles and energy products, and is the primary focus of analysis in the given text.", "\"NORTH AMERICA\":\"North America is the continent where Tesla Inc operates.\"", "\"ADAM JONAS, CFA\":\"Adam Jonas, CFA is an equity analyst who provided a stock rating, industry view, and price target for Tesla Inc.\"", "\"EVAN SILVERBERG, CFA, CPA\":\"Evan Silverberg, CFA, CPA is a research associate who contributed to the analysis of Tesla Inc.\"", "\"MODEL T\":\"Model T is a historical car model produced by Ford that underwent significant price deflation due to manufacturing innovations.\"", "\"HENRY FORD\":Henry Ford is a significant historical figure, known for his pioneering contributions to the manufacturing industry. In 1910, he introduced mass manufacturing at his Highland Park, Michigan factory, and three years later, in 1913, he unveiled the groundbreaking moving assembly line for the Model T. These innovations led to substantial levels of deflation in the automobile industry, demonstrating Ford's transformative impact on the sector. In summary, Henry Ford is associated with the introduction of mass manufacturing and the moving assembly line for the Model T, which brought about dramatic changes and efficiencies in the automotive industry.", "\"TESLA\":\"Tesla is an electric vehicle company that has recently reduced the price of its Model Y, making it competitive with a Toyota RAV4 Hybrid in terms of price.\"", "\"TOYOTA\":\"Toyota is a car manufacturer that produces the RAV4 Hybrid, which is a competitor to Tesla's Model Y in terms of price and features.\"", "\"HIGHLAND PARK, MICHIGAN\":\"Highland Park, Michigan is a location where Henry Ford introduced mass manufacturing in his factory in 1910.\"", "\"1910\":\"1910 is the year when Henry Ford introduced mass manufacturing in his Highland Park, Michigan factory.\"", "\"1913\":\"1913 is the year when Henry Ford introduced his world-changing moving assembly in his factory.\"", "\"MASS MANUFACTURING\":\"Mass Manufacturing is a concept introduced by Henry Ford in his Highland Park, Michigan factory in 1910, driving dramatic levels of deflation in the automobile industry.\"", "\"MOVING ASSEMBLY\":\"Moving Assembly is a concept introduced by Henry Ford in his factory in 1913, driving dramatic levels of deflation in the automobile industry.\"", "\"DRASTIC LEVELS OF DEFLATION\":\"Drastic Levels of Deflation is a concept that refers to the significant reduction in prices in the automobile industry due to innovation in manufacturing.\"", "\"REFINITIV\":\"Refinitiv is a source of data and information for the stock analysis provided in the text.\">)**"]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"INVESTMENT BANKING CLIENTS (IBC)\":\"Investment Banking Clients (IBC) are organizations that have paid investment banking fees to Morgan Stanley in the past 12 months.\"", "\"OTHER MATERIAL INVESTMENT SERVICES CLIENTS (MISC)\":\"Other Material Investment Services Clients (MISC) are organizations that receive other investment services from Morgan Stanley but have not paid investment banking fees in the past 12 months.\"", "\"OVERWEIGHT\":\"Overweight is a stock rating assigned by Morgan Stanley, indicating that the stock's total return is expected to exceed the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.\"", "\"NOT-RATED\":\"Not-Rated is a stock rating assigned by Morgan Stanley, indicating that the analyst does not have adequate conviction about the stock's total return relative to the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.\"", "\"BUY\":", "\"HOLD\":", "\"SELL\":", "\"RESEARCH ANALYST\":\"The Research Analyst is a person who contributes to the production of Morgan Stanley Research, with their work typically updated with a certain frequency.\"", "\"RESEARCH MANAGEMENT\":\"Research Management refers to the individuals who determine the publication schedule for Morgan Stanley Research based on current conditions.\"", "\"SECTION 975 OF THE DODD-FRANK WALL STREET REFORM AND CONSUMER PROTECTION ACT\":\"Section 975 is a legislative event that defines the meaning of advice within its context.\"", "\"TACTICAL IDEA\":\"A Tactical Idea is an equity research product from Morgan Stanley that may contain views contrary to recommendations or views expressed in other research on the same stock.\"", "\"MATRIX\":\"Matrix is a proprietary research portal from Morgan Stanley where clients can access research.\"", "\"SALES REPRESENTATIVE\":\"A Sales Representative is a person who can assist clients in accessing Morgan Stanley Research and related products.\"", "\"THIRD-PARTY VENDORS\":\"Third-Party Vendors are organizations that distribute Morgan Stanley Research to clients as a convenience.\""]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"SECULAR GROWTH\":\"Secular Growth is a concept, referenced as a risk reward theme.\">)**", "\"ELECTRIC VEHICLES\":\"Electric Vehicles is a concept, referenced as a risk reward theme.\">)**", "\"HISTORICAL STOCK PERFORMANCE\":", "\"CURRENT STOCK PRICE\":", "\"PRICE TARGET\":", "\"BULL CASE\":", "\"BASE CASE\":", "\"RISK REWARD THEMES\":", "\"MORGAN STANLEY\":Morgan Stanley is a financial services company that produces equity research and provides research, ratings, and investment services to clients. The company is subject to certain terms of use and privacy policies. As an investment bank, it corresponds its ratings to buy, hold, and sell recommendations for regulatory purposes. Morgan Stanley is also an organization that offers research, estimates, and opinions about various companies and sectors. It is the entity that has relationships with and provides services to multiple companies mentioned in the text.", "\"DIEGO ORTEGA LAYA\":\"Diego Ortega Laya is a person who owns securities of Ferrari NV and General Motors Company.\"", "\"FERRARI NV\":Ferrari NV is an organization that is part of Morgan Stanley's research coverage. As of December 30, 2022, Morgan Stanley beneficially owned 1% or more of a class of common equity securities of Ferrari NV. Additionally, Diego Ortega Laya owns securities of the company. Ferrari NV is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.", "\"GENERAL MOTORS COMPANY\":General Motors Company is an organization that is part of Morgan Stanley's research coverage. Morgan Stanley managed or co-managed a public offering of securities for General Motors in the last 12 months and beneficially owned 1% or more of a class of common equity securities as of December 30, 2022. Diego Ortega Laya owns securities in the company. Additionally, General Motors is one of the companies mentioned in the text that potentially receives compensation for products and services other than investment banking services from Morgan Stanley, and has a relationship with an employee, director, or consultant of Morgan Stanley.", "\"AMERICAN AXLE & MANUFACTURING HOLDINGS INC\":American Axle & Manufacturing Holdings Inc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. This company is one of those mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"APTIV PLC\":Aptiv Plc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. It is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"AUTONATION INC.\":AutoNation Inc. is an organization that is part of Morgan Stanley's research coverage and is one of the companies mentioned in the text, for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services.", "\"AVIS BUDGET GROUP INC\":Avis Budget Group Inc is an organization that is part of Morgan Stanley's research coverage. In the past 12 months, Morgan Stanley has managed or co-managed a public offering of securities for Avis Budget Group Inc, and Morgan Stanley expects to receive or intends to seek compensation for investment banking services from the company. This information is based on the text that mentions Avis Budget Group Inc as one of the companies potentially receiving or seeking compensation for investment banking services from Morgan Stanley."]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"BORGWARNER INC.\":BorgWarner Inc. is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. This company is one of the entities mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"CARVANA CO\":Carvana Co is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. The company is one of the entities mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"FISKER INC\":Fisker Inc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. This company is one of those mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"FORD MOTOR COMPANY\":Ford Motor Company is an organization that is part of Morgan Stanley's research coverage, for which Morgan Stanley managed or co-managed a public offering of securities in the last 12 months and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. Additionally, Ford Motor Company is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.", "\"HERTZ GLOBAL HOLDINGS INC\":Hertz Global Holdings Inc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. It is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley and having a relationship with an employee, director, or consultant of Morgan Stanley.", "\"LEAR CORPORATION\":Lear Corporation is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. Additionally, Lear Corporation is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.", "\"LI-CYCLE HOLDINGS CORP.\":Li-Cycle Holdings Corp. is an organization that is both part of Morgan Stanley's research coverage and one of the companies mentioned in the text. Morgan Stanley expects to receive or intends to seek compensation for investment banking services in relation to Li-Cycle Holdings Corp. This information indicates that there is a business relationship between the two entities, with Morgan Stanley providing investment banking services and conducting research on Li-Cycle Holdings Corp.", "\"LITHIA MOTORS INC.\":Lithia Motors Inc. is an organization that is part of Morgan Stanley's research coverage and is one of the companies mentioned in the text. Lithia Motors Inc. is expected to receive or has sought compensation for investment banking services from Morgan Stanley.", "\"MOBILEYE GLOBAL INC\":Mobileye Global Inc is an organization that is part of Morgan Stanley's research coverage, and it has been involved with Morgan Stanley in a public offering of securities within the last 12 months for which Morgan Stanley managed or co-managed and expects to receive or intends to seek compensation for investment banking services. The company is mentioned multiple times in the text, and it is one of the companies potentially relevant to Morgan Stanley's services, possibly receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"QUANTUMSCAPE CORP\":Quantumscape Corp is an organization that is part of Morgan Stanley's research coverage and is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley. This indicates that Quantumscape Corp has a business relationship with Morgan Stanley, where the financial institution provides investment banking services and possibly research coverage, and Quantumscape Corp may receive or seek compensation for these services.", "\"ERTZ GLOBAL HOLDINGS INC\":\"ertz Global Holdings Inc is one of the companies mentioned in the text, potentially relevant to Morgan Stanley's services.\"", "\"TESLA INC.\":\"Tesla Inc. is one of the companies mentioned in the text, potentially relevant to Morgan Stanley's services.\"", "\"LUCID GROUP INC\":\"Lucid Group Inc is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.\"", "\"MAGNA INTERNATIONAL INC.\":\"Magna International Inc. is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.\"", "\"RIVIAN AUTOMOTIVE, INC.\":\"Rivian Automotive, Inc. is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.\"", "\"VISTEON CORPORATION\":\"Visteon Corporation is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.\""]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"INVESTMENT BANKING CLIENTS (IBC)\":\"Investment Banking Clients (IBC) are organizations that have paid investment banking fees to Morgan Stanley in the past 12 months.\"", "\"OTHER MATERIAL INVESTMENT SERVICES CLIENTS (MISC)\":\"Other Material Investment Services Clients (MISC) are organizations that receive other investment services from Morgan Stanley but have not paid investment banking fees in the past 12 months.\"", "\"OVERWEIGHT\":\"Overweight is a stock rating assigned by Morgan Stanley, indicating that the stock's total return is expected to exceed the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.\"", "\"NOT-RATED\":\"Not-Rated is a stock rating assigned by Morgan Stanley, indicating that the analyst does not have adequate conviction about the stock's total return relative to the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.\"", "\"BUY\":", "\"HOLD\":", "\"SELL\":", "\"RESEARCH ANALYST\":\"The Research Analyst is a person who contributes to the production of Morgan Stanley Research, with their work typically updated with a certain frequency.\"", "\"RESEARCH MANAGEMENT\":\"Research Management refers to the individuals who determine the publication schedule for Morgan Stanley Research based on current conditions.\"", "\"SECTION 975 OF THE DODD-FRANK WALL STREET REFORM AND CONSUMER PROTECTION ACT\":\"Section 975 is a legislative event that defines the meaning of advice within its context.\"", "\"TACTICAL IDEA\":\"A Tactical Idea is an equity research product from Morgan Stanley that may contain views contrary to recommendations or views expressed in other research on the same stock.\"", "\"MATRIX\":\"Matrix is a proprietary research portal from Morgan Stanley where clients can access research.\"", "\"SALES REPRESENTATIVE\":\"A Sales Representative is a person who can assist clients in accessing Morgan Stanley Research and related products.\"", "\"THIRD-PARTY VENDORS\":\"Third-Party Vendors are organizations that distribute Morgan Stanley Research to clients as a convenience.\""]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"SECULAR GROWTH\":\"Secular Growth is a concept, referenced as a risk reward theme.\">)**", "\"ELECTRIC VEHICLES\":\"Electric Vehicles is a concept, referenced as a risk reward theme.\">)**", "\"HISTORICAL STOCK PERFORMANCE\":", "\"CURRENT STOCK PRICE\":", "\"PRICE TARGET\":", "\"BULL CASE\":", "\"BASE CASE\":", "\"RISK REWARD THEMES\":", "\"MORGAN STANLEY\":Morgan Stanley is a financial services company that produces equity research and provides research, ratings, and investment services to clients. The company is subject to certain terms of use and privacy policies. As an investment bank, it corresponds its ratings to buy, hold, and sell recommendations for regulatory purposes. Morgan Stanley is also an organization that offers research, estimates, and opinions about various companies and sectors. It is the entity that has relationships with and provides services to multiple companies mentioned in the text.", "\"DIEGO ORTEGA LAYA\":\"Diego Ortega Laya is a person who owns securities of Ferrari NV and General Motors Company.\"", "\"FERRARI NV\":Ferrari NV is an organization that is part of Morgan Stanley's research coverage. As of December 30, 2022, Morgan Stanley beneficially owned 1% or more of a class of common equity securities of Ferrari NV. Additionally, Diego Ortega Laya owns securities of the company. Ferrari NV is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.", "\"GENERAL MOTORS COMPANY\":General Motors Company is an organization that is part of Morgan Stanley's research coverage. Morgan Stanley managed or co-managed a public offering of securities for General Motors in the last 12 months and beneficially owned 1% or more of a class of common equity securities as of December 30, 2022. Diego Ortega Laya owns securities in the company. Additionally, General Motors is one of the companies mentioned in the text that potentially receives compensation for products and services other than investment banking services from Morgan Stanley, and has a relationship with an employee, director, or consultant of Morgan Stanley.", "\"AMERICAN AXLE & MANUFACTURING HOLDINGS INC\":American Axle & Manufacturing Holdings Inc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. This company is one of those mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"APTIV PLC\":Aptiv Plc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. It is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"AUTONATION INC.\":AutoNation Inc. is an organization that is part of Morgan Stanley's research coverage and is one of the companies mentioned in the text, for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services.", "\"AVIS BUDGET GROUP INC\":Avis Budget Group Inc is an organization that is part of Morgan Stanley's research coverage. In the past 12 months, Morgan Stanley has managed or co-managed a public offering of securities for Avis Budget Group Inc, and Morgan Stanley expects to receive or intends to seek compensation for investment banking services from the company. This information is based on the text that mentions Avis Budget Group Inc as one of the companies potentially receiving or seeking compensation for investment banking services from Morgan Stanley."]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"SECULAR GROWTH\":\"Secular Growth is a concept, referenced as a risk reward theme.\">)**", "\"ELECTRIC VEHICLES\":\"Electric Vehicles is a concept, referenced as a risk reward theme.\">)**", "\"HISTORICAL STOCK PERFORMANCE\":", "\"CURRENT STOCK PRICE\":", "\"PRICE TARGET\":", "\"BULL CASE\":", "\"BASE CASE\":", "\"RISK REWARD THEMES\":", "\"MORGAN STANLEY\":Morgan Stanley is a financial services company that produces equity research and provides research, ratings, and investment services to clients. The company is subject to certain terms of use and privacy policies. As an investment bank, it corresponds its ratings to buy, hold, and sell recommendations for regulatory purposes. Morgan Stanley is also an organization that offers research, estimates, and opinions about various companies and sectors. It is the entity that has relationships with and provides services to multiple companies mentioned in the text.", "\"DIEGO ORTEGA LAYA\":\"Diego Ortega Laya is a person who owns securities of Ferrari NV and General Motors Company.\"", "\"FERRARI NV\":Ferrari NV is an organization that is part of Morgan Stanley's research coverage. As of December 30, 2022, Morgan Stanley beneficially owned 1% or more of a class of common equity securities of Ferrari NV. Additionally, Diego Ortega Laya owns securities of the company. Ferrari NV is one of the companies mentioned in the text, potentially receiving compensation for products and services other than investment banking services from Morgan Stanley.", "\"GENERAL MOTORS COMPANY\":General Motors Company is an organization that is part of Morgan Stanley's research coverage. Morgan Stanley managed or co-managed a public offering of securities for General Motors in the last 12 months and beneficially owned 1% or more of a class of common equity securities as of December 30, 2022. Diego Ortega Laya owns securities in the company. Additionally, General Motors is one of the companies mentioned in the text that potentially receives compensation for products and services other than investment banking services from Morgan Stanley, and has a relationship with an employee, director, or consultant of Morgan Stanley.", "\"AMERICAN AXLE & MANUFACTURING HOLDINGS INC\":American Axle & Manufacturing Holdings Inc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. This company is one of those mentioned in the text, and it is potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"APTIV PLC\":Aptiv Plc is an organization that is part of Morgan Stanley's research coverage and for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services. It is one of the companies mentioned in the text, potentially receiving or seeking compensation for investment banking services from Morgan Stanley.", "\"AUTONATION INC.\":AutoNation Inc. is an organization that is part of Morgan Stanley's research coverage and is one of the companies mentioned in the text, for which Morgan Stanley expects to receive or intends to seek compensation for investment banking services.", "\"AVIS BUDGET GROUP INC\":Avis Budget Group Inc is an organization that is part of Morgan Stanley's research coverage. In the past 12 months, Morgan Stanley has managed or co-managed a public offering of securities for Avis Budget Group Inc, and Morgan Stanley expects to receive or intends to seek compensation for investment banking services from the company. This information is based on the text that mentions Avis Budget Group Inc as one of the companies potentially receiving or seeking compensation for investment banking services from Morgan Stanley."]}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n    return await self._connection.handle_async_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1558, in _request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 36, in _execute_llm\n    embedding = await self.client.embeddings.create(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 215, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1826, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1577, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": ["\"INVESTMENT BANKING CLIENTS (IBC)\":\"Investment Banking Clients (IBC) are organizations that have paid investment banking fees to Morgan Stanley in the past 12 months.\"", "\"OTHER MATERIAL INVESTMENT SERVICES CLIENTS (MISC)\":\"Other Material Investment Services Clients (MISC) are organizations that receive other investment services from Morgan Stanley but have not paid investment banking fees in the past 12 months.\"", "\"OVERWEIGHT\":\"Overweight is a stock rating assigned by Morgan Stanley, indicating that the stock's total return is expected to exceed the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.\"", "\"NOT-RATED\":\"Not-Rated is a stock rating assigned by Morgan Stanley, indicating that the analyst does not have adequate conviction about the stock's total return relative to the average total return of the analyst's industry coverage universe on a risk-adjusted basis over the next 12-18 months.\"", "\"BUY\":", "\"HOLD\":", "\"SELL\":", "\"RESEARCH ANALYST\":\"The Research Analyst is a person who contributes to the production of Morgan Stanley Research, with their work typically updated with a certain frequency.\"", "\"RESEARCH MANAGEMENT\":\"Research Management refers to the individuals who determine the publication schedule for Morgan Stanley Research based on current conditions.\"", "\"SECTION 975 OF THE DODD-FRANK WALL STREET REFORM AND CONSUMER PROTECTION ACT\":\"Section 975 is a legislative event that defines the meaning of advice within its context.\"", "\"TACTICAL IDEA\":\"A Tactical Idea is an equity research product from Morgan Stanley that may contain views contrary to recommendations or views expressed in other research on the same stock.\"", "\"MATRIX\":\"Matrix is a proprietary research portal from Morgan Stanley where clients can access research.\"", "\"SALES REPRESENTATIVE\":\"A Sales Representative is a person who can assist clients in accessing Morgan Stanley Research and related products.\"", "\"THIRD-PARTY VENDORS\":\"Third-Party Vendors are organizations that distribute Morgan Stanley Research to clients as a convenience.\""]}}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 162, in execute_with_retry\n    await self._rate_limiter.acquire(input_tokens)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\limiting\\tpm_rpm_limiter.py\", line 32, in acquire\n    await self._tpm_limiter.acquire(num_tokens)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\aiolimiter\\leakybucket.py\", line 95, in acquire\n    raise ValueError(\"Can't acquire more than the maximum capacity\")\nValueError: Can't acquire more than the maximum capacity\n", "source": "Can't acquire more than the maximum capacity", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 162, in execute_with_retry\n    await self._rate_limiter.acquire(input_tokens)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\graphrag\\llm\\limiting\\tpm_rpm_limiter.py\", line 32, in acquire\n    await self._tpm_limiter.acquire(num_tokens)\n  File \"D:\\GraphRAG-test\\graphragvenv\\Lib\\site-packages\\aiolimiter\\leakybucket.py\", line 95, in acquire\n    raise ValueError(\"Can't acquire more than the maximum capacity\")\nValueError: Can't acquire more than the maximum capacity\n", "source": "Can't acquire more than the maximum capacity", "details": null}
